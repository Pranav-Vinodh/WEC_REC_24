{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9124431,"sourceType":"datasetVersion","datasetId":5508415}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-11T17:05:07.185363Z","iopub.execute_input":"2024-08-11T17:05:07.185930Z","iopub.status.idle":"2024-08-11T17:05:07.196927Z","shell.execute_reply.started":"2024-08-11T17:05:07.185865Z","shell.execute_reply":"2024-08-11T17:05:07.195943Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"/kaggle/input/fakereddit/multimodal_validate.tsv\n/kaggle/input/fakereddit/multimodal_train.tsv\n/kaggle/input/fakereddit/multimodal_test_public.tsv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fakeddit Multimodal Fake News Detection Challenge","metadata":{}},{"cell_type":"markdown","source":"The Main objective is to create a multiModal model capable of detecting fake news by extracting the information from text, title and image.","metadata":{}},{"cell_type":"markdown","source":"# Importing the required libraries","metadata":{}},{"cell_type":"code","source":"#importing the core libraries required for the task\nimport os\nimport numpy as np\nimport keras\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom PIL import Image, ImageFile, UnidentifiedImageError","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:07.198525Z","iopub.execute_input":"2024-08-11T17:05:07.198801Z","iopub.status.idle":"2024-08-11T17:05:07.210492Z","shell.execute_reply.started":"2024-08-11T17:05:07.198778Z","shell.execute_reply":"2024-08-11T17:05:07.209724Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Importing Pytorch and transformers\nimport torch\n# Import nn module for building stacked layers and optimizers\nfrom torch import nn, optim\nimport torchvision\nfrom torchvision import datasets, models, transforms\n# Import modules for dataset configuration and loading\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.optim.lr_scheduler as lr_scheduler\n\n#importing visual libraries\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:07.211477Z","iopub.execute_input":"2024-08-11T17:05:07.211724Z","iopub.status.idle":"2024-08-11T17:05:07.221840Z","shell.execute_reply.started":"2024-08-11T17:05:07.211703Z","shell.execute_reply":"2024-08-11T17:05:07.221048Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Importing model evaluation tools\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport sklearn\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:07.224517Z","iopub.execute_input":"2024-08-11T17:05:07.225053Z","iopub.status.idle":"2024-08-11T17:05:07.232020Z","shell.execute_reply.started":"2024-08-11T17:05:07.225028Z","shell.execute_reply":"2024-08-11T17:05:07.231169Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Initializing and Preprocessing the Fakeddit-Benchmark dataset ","metadata":{}},{"cell_type":"code","source":"# Helper function to read the tsv file and convert it into a dataframe. \n# We use the read.csv() method and provide delimiter as '/t' to let the function know its a tsv file\ndef initialize_dataframe(path, file):\n    dataframe = pd.read_csv(os.path.join(path, file), delimiter=\"\\t\")\n    \n    # Dropping the redundant columns \n    if \"Unnamed: 0\" in dataframe.columns:\n        dataframe = dataframe.drop([\"Unnamed: 0\"], 1)\n    \n    # Return dataframe\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:07.232994Z","iopub.execute_input":"2024-08-11T17:05:07.233249Z","iopub.status.idle":"2024-08-11T17:05:07.242860Z","shell.execute_reply.started":"2024-08-11T17:05:07.233227Z","shell.execute_reply":"2024-08-11T17:05:07.242055Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Read input into dataframes\ndf_test = initialize_dataframe(\"dataset\", \"/kaggle/input/fakereddit/multimodal_test_public.tsv\")\ndf_validate = initialize_dataframe(\"dataset\", \"/kaggle/input/fakereddit/multimodal_train.tsv\")\ndf_train = initialize_dataframe(\"dataset\", \"/kaggle/input/fakereddit/multimodal_validate.tsv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:07.243881Z","iopub.execute_input":"2024-08-11T17:05:07.244181Z","iopub.status.idle":"2024-08-11T17:05:12.104191Z","shell.execute_reply.started":"2024-08-11T17:05:07.244157Z","shell.execute_reply":"2024-08-11T17:05:12.103171Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**As The training dataset contains over 700,000 samples, we use train-test-split to first use only 20%-30% of the total training data for actually training the model. Then we further divide the training data normally into a 80-20 split, keeping 80% for training and rest 20% for testing purposes. \nStratify function is applied in order to keep the per class sample distribution from original Fakeddit source dataset.**","metadata":{}},{"cell_type":"code","source":"# Splitting complete Fakeddit-dataset into 20% training dataframe\n# and 80% backup dataframe\ndf_train, df_backup = train_test_split(\n    df_train,\n    test_size=0.8,\n    shuffle=True,\n    \n   stratify = df_train[\"6_way_label\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.105457Z","iopub.execute_input":"2024-08-11T17:05:12.105755Z","iopub.status.idle":"2024-08-11T17:05:12.199462Z","shell.execute_reply.started":"2024-08-11T17:05:12.105730Z","shell.execute_reply":"2024-08-11T17:05:12.198554Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Keeping 80% of data samples for training and 20% for testing purposes\ndf_train, df_test = train_test_split(\n    df_train,\n    test_size=0.2,\n    shuffle=True,\n   \n    stratify=df_train[\"6_way_label\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.200558Z","iopub.execute_input":"2024-08-11T17:05:12.200843Z","iopub.status.idle":"2024-08-11T17:05:12.226433Z","shell.execute_reply.started":"2024-08-11T17:05:12.200818Z","shell.execute_reply":"2024-08-11T17:05:12.225692Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Dividing test split dataframe by factor 0,5 to have identically\n# sized splits for validation and testing\ndf_test, df_validate = train_test_split(\n    df_test,\n    test_size=0.5,\n    shuffle=True,\n   \n    stratify=df_test[\"6_way_label\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.227459Z","iopub.execute_input":"2024-08-11T17:05:12.227730Z","iopub.status.idle":"2024-08-11T17:05:12.329686Z","shell.execute_reply.started":"2024-08-11T17:05:12.227706Z","shell.execute_reply":"2024-08-11T17:05:12.328993Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Fake News subtypes in order of Fakeddit benchmark dataset labeling\nCLASS_NAMES = [\"True\", \"Satire\", \"False Conn.\", \"Impost. Content\", \"Man. Content\", \"Mis. Content\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.333522Z","iopub.execute_input":"2024-08-11T17:05:12.333855Z","iopub.status.idle":"2024-08-11T17:05:12.337838Z","shell.execute_reply.started":"2024-08-11T17:05:12.333832Z","shell.execute_reply":"2024-08-11T17:05:12.336878Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**We will be using the DistillBert Model to process the text and title information**","metadata":{}},{"cell_type":"code","source":"# Importing needed modules for DistilBert model\nfrom transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n\n# Loading DistilBert tokenizers adjusted for lower case English text corpus\n# for tokenization of title input sequence\ntitle_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.339049Z","iopub.execute_input":"2024-08-11T17:05:12.339321Z","iopub.status.idle":"2024-08-11T17:05:12.452555Z","shell.execute_reply.started":"2024-08-11T17:05:12.339299Z","shell.execute_reply":"2024-08-11T17:05:12.451839Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Processing the input data","metadata":{}},{"cell_type":"markdown","source":"** PostDataset contains all relevant information per batch and DataLoader iterates over complete Dataset to feed batches of size 16 to model.\nInitially it doesn't contain any information regarding the reddit images or titles**","metadata":{}},{"cell_type":"code","source":"class PostDataset(Dataset):\n    \n    # Constructor initialized with relevant attributes plus tokenizer information\n    def __init__(self, post_id, title, label, title_tokenizer, max_len):\n        self.post_id = post_id\n        self.title = title\n        self.label = label\n        self.title_tokenizer = title_tokenizer\n        self.max_length = max_len\n        \n    # Returns length of the dataset for internal looping \n    def __len__(self):\n        return len(self.label)\n    \n    # Internal function to fetch next sample within dataset object\n    def __getitem__(self, idx):\n        # Iteration function to retireve next sample\n        post_id = self.post_id[idx]\n        title = self.title[idx]\n        label = self.label[idx]\n\n        # Saving id, clean_title and label entries per post\n        # in sample dictionary\n        sample = {\n            \"post_id\": post_id,\n            \"clean_title\": title,\n            \"label\": label\n        }\n        \n        # Return sample dictionary containing all needed attributes\n        return sample","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.453494Z","iopub.execute_input":"2024-08-11T17:05:12.453760Z","iopub.status.idle":"2024-08-11T17:05:12.460775Z","shell.execute_reply.started":"2024-08-11T17:05:12.453738Z","shell.execute_reply":"2024-08-11T17:05:12.459909Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"**Train transform is specifically used to process image data for the train_model() function.Additional data augmentation is performed by random crop resizing and flipping image horizontally in order to artificially inflate the underlying training set split**","metadata":{}},{"cell_type":"code","source":"# Transform function for image processing (training)\n# Performing data augmentation by random resizing, cropping\n# and flipping images in order to artificially create new\n# image data per training epoch\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.255]\n    )\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.461845Z","iopub.execute_input":"2024-08-11T17:05:12.462165Z","iopub.status.idle":"2024-08-11T17:05:12.475668Z","shell.execute_reply.started":"2024-08-11T17:05:12.462141Z","shell.execute_reply":"2024-08-11T17:05:12.474842Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**Internal collate_batch() function implements the processing / sample preparation logic to convert all information into a readable format for the neural network model. Per batch a total of 16 samples are fetched, whereas the images are processed and attributes post_id, image tensor, and 6-way-label are stacked within a single batch dictionary. One batch holds above mentioned information for 16 fetched samples. batch_size of 16 was chosen in alignment with previously used DistilBert configuration.**","metadata":{}},{"cell_type":"code","source":"def collate_batch(batch):\n    \n    # List to save processed batch samples\n    batch_processed = []\n    \n    # Iteration over input batch of size 16\n    for i in range(len(batch)):\n        \n        # Saving attributes in local variables\n        post_id = batch[i][\"post_id\"]\n        title = batch[i][\"clean_title\"]\n        label = batch[i][\"label\"]\n        \n        # Leveraging DistilBertTokenizer to generate\n        # encoding of input text sequence\n        encoding = title_tokenizer.encode_plus(\n            title,\n            max_length=80,\n            padding=\"max_length\",\n            truncation=True,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        # Try-Except-Else clause to process image data\n        # Fetch images from image_set folder via post_id, transform and reshape tensor\n        try:\n            image_path = df_train.iloc[i][\"image_url\"]\n            image = Image.open(image_path)\n        # Handling FileNotFoundError and randomly initializing pixels\n        except FileNotFoundError:\n            image = torch.rand(3, 224, 224)\n            image = torch.unsqueeze(image, 0)\n        # Handling UnidentifiedImageError and randomly initializing pixels\n        except UnidentifiedImageError:\n            image = torch.rand(3, 224, 224)\n            image = torch.unsqueeze(image, 0)\n        # Handling OSError and randomly initializing pixels\n        except OSError:\n            image = torch.rand(3, 224, 224)\n            image = torch.unsqueeze(image, 0)\n        # Else: Convert image to RGB, process with train_transform\n        # and reshape to tensor of shape = [1, 3, 224, 224] for\n        # [sample_count, color_channels, height in pixel, width in pixel]\n        else:\n            image = image.convert(\"RGB\")\n            image = train_transform(image)\n            image = torch.unsqueeze(image, 0)\n        \n\n        # Storing processed attributes of sample in sample\n        # dictionary: post_id, title (text), input_ids,\n        # attention_mask, image and label\n        sample = {\n            \"post_id\": post_id,\n            \"title\": title,\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"image\": image.flatten(),\n            \"label\": torch.tensor(label, dtype=torch.long)\n        }\n        \n        # Append current samples dictionary to processed\n        # batch list --> List of sample dictionaries\n        batch_processed.append(sample)\n        \n    # Complex operation in order to unpack list of dictionaries and\n    # merge dictionary entries into correct PyTorch tensor for forward processing\n    postId = []\n    titles = []\n    \n    # For-loop to stack sample dictionary keys into appropriate format\n    for i in range(len(batch_processed)):\n        # If first sample of batch, initialize attribute tensors and reshape\n        if i == 0:\n            postId.append(batch_processed[i][\"post_id\"])\n            titles.append(batch_processed[i][\"title\"])\n            input_ids_tensor = batch_processed[i][\"input_ids\"].reshape(-1, 80)\n            attention_mask_tensor = batch_processed[i][\"attention_mask\"].reshape(-1, 80)\n            image_tensor = batch_processed[i][\"image\"].reshape(-1, 3, 224, 224)\n            label_tensor = batch_processed[i][\"label\"].reshape(-1,)\n            continue\n\n        # Stack attributes of sample dictionary keys to generate correct tensor shape\n        postId.append(batch_processed[i][\"post_id\"])\n        titles.append(batch_processed[i][\"title\"])\n        input_ids_tensor = torch.cat((input_ids_tensor, batch_processed[i][\"input_ids\"].reshape(-1, 80)))\n        attention_mask_tensor = torch.cat((attention_mask_tensor, batch_processed[i][\"attention_mask\"].reshape(-1, 80)))\n        image_tensor = torch.cat((image_tensor, batch_processed[i][\"image\"].reshape(-1, 3, 224, 224)))\n        label_tensor = torch.cat((label_tensor, batch_processed[i][\"label\"].reshape(-1,)))\n    \n    # Returning batch list of sample dictionaries containing 16 processed samples\n    return {\n        \"post_id\": postId,\n        \"title\": titles,\n        \"input_ids\": input_ids_tensor,\n        \"attention_mask\": attention_mask_tensor,\n        \"image\": image_tensor,\n        \"label\": label_tensor\n    }","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.476764Z","iopub.execute_input":"2024-08-11T17:05:12.477093Z","iopub.status.idle":"2024-08-11T17:05:12.495090Z","shell.execute_reply.started":"2024-08-11T17:05:12.477070Z","shell.execute_reply":"2024-08-11T17:05:12.494204Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def create_data_loader(df, title_tokenizer, max_len, batch_size):\n    \n    # Initialization of PostDataset and assignment\n    # to dataset variable\n    dataset = PostDataset(\n                post_id = df[\"id\"].to_numpy(),\n                title = df[\"clean_title\"].to_numpy(),\n                label = df[\"6_way_label\"].to_numpy(),\n                title_tokenizer = title_tokenizer,\n                max_len = max_len\n              )\n    \n    # Forwarding dataset variable, batch_size and collate_batch function\n    # to PyTorch DataLoader module. Returns Iterable DataLoader object\n    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_batch, num_workers=2, pin_memory=True, prefetch_factor=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.496261Z","iopub.execute_input":"2024-08-11T17:05:12.496525Z","iopub.status.idle":"2024-08-11T17:05:12.509435Z","shell.execute_reply.started":"2024-08-11T17:05:12.496497Z","shell.execute_reply":"2024-08-11T17:05:12.508555Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"**Now we repeat the same process for the testing and validation data**","metadata":{}},{"cell_type":"code","source":"# Transform function for image processing (validation and testing)\n# No data augmentation in validation and test data splits in order to\n# define constant validation and testing process\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.255]\n    )\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.510477Z","iopub.execute_input":"2024-08-11T17:05:12.510729Z","iopub.status.idle":"2024-08-11T17:05:12.522920Z","shell.execute_reply.started":"2024-08-11T17:05:12.510708Z","shell.execute_reply":"2024-08-11T17:05:12.522133Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def collate_batch_val_test(batch):\n    \n    # List to save processed batch samples\n    batch_processed = []\n    \n    # Iteration over input batch\n    for i in range(len(batch)):\n        \n        # Iteration over input batch of size 16\n        post_id = batch[i][\"post_id\"]\n        title = batch[i][\"clean_title\"]\n        label = batch[i][\"label\"]\n        \n        # Leveraging DistilBertTokenizer to generate\n        # encoding of input text sequence\n        encoding = title_tokenizer.encode_plus(\n            title,\n            max_length=80,\n            padding=\"max_length\",\n            truncation=True,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        # Try-Except-Else clause to process image data\n        # Fetch images from image_set folder via post_id, transform and reshape tensor\n        try:\n            image_paths = df_test.iloc[i][\"image_url\"]\n            image = Image.open(image_paths)\n        # Handling FileNotFoundError and randomly initializing pixels\n        except FileNotFoundError:\n            image = torch.rand(3, 224, 224)\n            image = torch.unsqueeze(image, 0)\n        # Handling UnidentifiedImageError and randomly initializing pixels\n        except UnidentifiedImageError:\n            image = torch.rand(3, 224, 224)\n            image = torch.unsqueeze(image, 0)\n        # Handling OSError and randomly initializing pixels\n        except OSError:\n            image = torch.rand(3, 224, 224)\n            image = torch.unsqueeze(image, 0)\n        # Else: Convert image to RGB, process with train_transform\n        # and reshape to tensor of shape = [1, 3, 224, 224] for\n        # [sample_count, color_channels, height in pixel, width in pixel]\n        else:\n            image = image.convert(\"RGB\")\n            image = train_transform(image)\n            image = torch.unsqueeze(image, 0)\n        \n\n        # Storing processed attributes of sample in sample\n        # dictionary: post_id, title (text), input_ids,\n        # attention_mask, image and label\n        sample = {\n            \"post_id\": post_id,\n            \"title\": title,\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"image\": image.flatten(),\n            \"label\": torch.tensor(label, dtype=torch.long)\n        }\n        \n        # Append current samples dictionary to processed\n        # batch list --> List of sample dictionaries\n        batch_processed.append(sample)\n        \n    # Complex operation in order to unpack list of dictionaries and\n    # merge dictionary entries into correct PyTorch tensor for forward processing\n    postId = []\n    titles = []\n    \n    # For-loop to stack sample dictionary keys into appropriate format\n    for i in range(len(batch_processed)):\n        if i == 0:\n            # If first sample of batch, initialize attribute tensors and reshape\n            postId.append(batch_processed[i][\"post_id\"])\n            titles.append(batch_processed[i][\"title\"])\n            input_ids_tensor = batch_processed[i][\"input_ids\"].reshape(-1, 80)\n            attention_mask_tensor = batch_processed[i][\"attention_mask\"].reshape(-1, 80)\n            image_tensor = batch_processed[i][\"image\"].reshape(-1, 3, 224, 224)\n            label_tensor = batch_processed[i][\"label\"].reshape(-1,)\n            continue\n\n        # Stack attributes of sample dictionary keys to generate correct tensor shape\n        postId.append(batch_processed[i][\"post_id\"])\n        titles.append(batch_processed[i][\"title\"])\n        input_ids_tensor = torch.cat((input_ids_tensor, batch_processed[i][\"input_ids\"].reshape(-1, 80)))\n        attention_mask_tensor = torch.cat((attention_mask_tensor, batch_processed[i][\"attention_mask\"].reshape(-1, 80)))\n        image_tensor = torch.cat((image_tensor, batch_processed[i][\"image\"].reshape(-1, 3, 224, 224)))\n        label_tensor = torch.cat((label_tensor, batch_processed[i][\"label\"].reshape(-1,)))\n    \n    # Returning batch list of sample dictionaries containing 16 processed samples\n    return {\n        \"post_id\": postId,\n        \"title\": titles,\n        \"input_ids\": input_ids_tensor,\n        \"attention_mask\": attention_mask_tensor,\n        \"image\": image_tensor,\n        \"label\": label_tensor\n    }","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.524118Z","iopub.execute_input":"2024-08-11T17:05:12.524488Z","iopub.status.idle":"2024-08-11T17:05:12.541407Z","shell.execute_reply.started":"2024-08-11T17:05:12.524465Z","shell.execute_reply":"2024-08-11T17:05:12.540591Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def val_test_create_data_loader(df, title_tokenizer, max_len, batch_size):\n    \n    # Initialization of PostTitleDataset and assignment\n    # to dataset variable\n    dataset = PostDataset(\n                post_id = df[\"id\"].to_numpy(),\n                title = df[\"clean_title\"].to_numpy(),\n                label = df[\"6_way_label\"].to_numpy(),\n                title_tokenizer = title_tokenizer,\n                max_len = max_len\n              )\n    \n    # Forwarding dataset variable, batch_size and collate function\n    # to Pytorch DataLoader module. DataLoader is returned, over which\n    # can be iterated\n    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_batch_val_test, num_workers=2, pin_memory=True, prefetch_factor=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.542338Z","iopub.execute_input":"2024-08-11T17:05:12.542589Z","iopub.status.idle":"2024-08-11T17:05:12.555829Z","shell.execute_reply.started":"2024-08-11T17:05:12.542568Z","shell.execute_reply":"2024-08-11T17:05:12.555125Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"%%time\n# Defining batch size and maximum sequence length\n# MAX_LEN is defined based on plotting of token length dsitribution\nBATCH_SIZE = 16\nMAX_LEN = 80\n\n# Initializing Pytorch DataLoader for train, validate and test split dataframes\ntrain_data_loader = create_data_loader(df_train, title_tokenizer, MAX_LEN, BATCH_SIZE)\nvalidate_data_loader = val_test_create_data_loader(df_validate, title_tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = val_test_create_data_loader(df_test, title_tokenizer, MAX_LEN, BATCH_SIZE)\n\n# Retrieving first batch from dataloaders via next() and iter() functions\ntrain_data = next(iter(train_data_loader))\nvalidate_data = next(iter(validate_data_loader))\ntest_data = next(iter(test_data_loader))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:12.556753Z","iopub.execute_input":"2024-08-11T17:05:12.557008Z","iopub.status.idle":"2024-08-11T17:05:13.680178Z","shell.execute_reply.started":"2024-08-11T17:05:12.556987Z","shell.execute_reply":"2024-08-11T17:05:13.678994Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 74.6 ms, sys: 285 ms, total: 360 ms\nWall time: 1.11 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Title-Image DistilFND model definition","metadata":{}},{"cell_type":"code","source":"class FakeNewsDetector(nn.Module):\n    \n    def __init__(self, num_classes):\n        super(FakeNewsDetector, self).__init__()\n        \n        # Initialize DistilBert model for title feature extraction\n        self.title_module = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        \n        # Initialize ResNet34 model for image feature extraction\n        self.image_module = models.resnet34(weights=True)\n        \n        # Dropout layer to randomly nullify 30% of elements of output tensors (useful during training)\n        self.drop = nn.Dropout(p=0.3)\n\n        # Fully connected layers to reshape the output tensors\n        self.fc_title = nn.Linear(in_features=self.title_module.config.hidden_size, out_features=num_classes, bias=True)\n        self.fc_image = nn.Linear(in_features=1000, out_features=num_classes, bias=True)\n\n        # Softmax layer for final class probability prediction\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, title_input_ids, title_attention_mask, image):\n        # Extract features from the title using DistilBert\n        title_last_hidden_states = self.title_module(\n            input_ids=title_input_ids,\n            attention_mask=title_attention_mask,\n            return_dict=False\n        )\n        # Extract the pooled output from the hidden states (CLS token)\n        title_pooled_output = title_last_hidden_states[0][:, 0, :]\n        # Apply dropout to the pooled output\n        title_pooled_output = self.drop(title_pooled_output)\n        \n        # Pass the title features through the fully connected layer\n        title_output = self.fc_title(title_pooled_output)\n\n        # Extract features from the image using ResNet34\n        image_output = self.image_module(image)\n        # Apply dropout to the image output\n        image_output = self.drop(image_output)\n        \n        # Pass the image features through the fully connected layer\n        image_output = self.fc_image(image_output)\n\n        # Combine the title and image features using element-wise maximum\n        fusion = torch.maximum(title_output, image_output)\n        \n        # Apply Softmax to the combined features to get class probabilities\n        return self.softmax(fusion)\n\n# Initialize the FakeNewsDetector with the number of classes\nfn_detector = FakeNewsDetector(num_classes=len(CLASS_NAMES))\n# Move the model to the specified device (e.g., GPU)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:13.682289Z","iopub.execute_input":"2024-08-11T17:05:13.682696Z","iopub.status.idle":"2024-08-11T17:05:14.442330Z","shell.execute_reply.started":"2024-08-11T17:05:13.682659Z","shell.execute_reply":"2024-08-11T17:05:14.441293Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* This code defines a PyTorch neural network model named FakeNewsDetector designed to detect fake news by combining textual and visual information. \n\nKey Components:\n* We are using the following models and layers in this function:\n* DistilBert Model: Loaded with pre-trained weights to extract features from text (titles).\n* ResNet34 Model: Loaded with pre-trained weights to extract features from images.\n* Dropout Layer: Helps in preventing overfitting during training by randomly setting a fraction of input units to zero.\n* Fully Connected Layers:\n    * fc_title: Reduces the dimensionality of the text feature vector to match the number of classes.\n    * fc_image: Reduces the dimensionality of the image feature vector to match the number of classes.\n    * Softmax Layer: Converts the final output to a probability distribution over the classes.\n        \n* Forward Pass (forward method):\n\n* Text Feature Extraction:\n    * The model takes the title_input_ids and title_attention_mask as inputs and processes them using DistilBert.\n    * Extracts the [CLS] token representation from the last hidden states of DistilBert, which serves as a summary of the input text.\n    * Applies dropout to the extracted text features.\n    * Passes the features through a fully connected layer to map them to the number of classes.\n* Image Feature Extraction:\n    * The model processes the input image using ResNet34 to extract features.\n    * Applies dropout to the extracted image features.\n    * Passes the features through a fully connected layer to map them to the number of classes.\n* Feature Fusion:\n    * Combines the text and image features using element-wise maximum operation.\n    * Applies the softmax function to the combined features to obtain class probabilities.\n    \n* Model Initialization and Device Assignment:\n\n    * Initializes the FakeNewsDetector with the specified number of classes.\n    * Moves the model to the specified computing device (e.g., GPU) for efficient computation.\n* Flow Summary:\n* The model combines features extracted from text (using DistilBert) and images (using ResNet34).\nThese features are processed through fully connected layers and combined.\nThe combined features are then passed through a softmax layer to predict the probabilities of each class, which represent whether the news is fake or not.\nThe model is initialized and moved to the specified device for training or inference.","metadata":{}},{"cell_type":"markdown","source":"**get_class_weights() function calculates percentage values per class for a weighted CrossEntropy. Reasoning is an highly imbalanced Fakeddit benchmarks dataset. Classes with a high number of Reddit-Post samples are normalized and are considered less by percentage during loss calculation. Also, some classes have considerably more sample data, all classes are weighted and taken as input into the loss calculation according to their respective number of samples. High number of class samples yields lower percentage weights, and low number of class samples yields higher percentage weight.**","metadata":{}},{"cell_type":"code","source":"# Printing model architecture\nprint(fn_detector)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.443599Z","iopub.execute_input":"2024-08-11T17:05:14.443929Z","iopub.status.idle":"2024-08-11T17:05:14.450750Z","shell.execute_reply.started":"2024-08-11T17:05:14.443878Z","shell.execute_reply":"2024-08-11T17:05:14.449934Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"FakeNewsDetector(\n  (title_module): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (image_module): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=512, out_features=1000, bias=True)\n  )\n  (drop): Dropout(p=0.3, inplace=False)\n  (fc_title): Linear(in_features=768, out_features=6, bias=True)\n  (fc_image): Linear(in_features=1000, out_features=6, bias=True)\n  (softmax): Softmax(dim=1)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_class_weights(dataframe):\n    \n    # Count labels per class / subtype of Fake News in training set split\n    # in sorted order 0, 1, 2, 3, 4, 5 and put into label_count list\n    # First, compute the value counts once\n    value_counts_sorted = dataframe[\"6_way_label\"].value_counts().sort_index()\n\n# Then, access the counts directly\n    label_count = [value_counts_sorted.get(i, 0) for i in range(6)]\n\n    # Calculate weights per class by subtracting from 1 label_count per class divided\n    # by sum of all label_counts\n    class_weights = [1 - (x / sum(label_count)) for x in label_count]\n    # Converting list of class_weights to float PyTorch tensor and assigning to device\n    class_weights = torch.FloatTensor(class_weights)\n\n    # Returns class_weights tensor of data type float\n    return class_weights","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.452077Z","iopub.execute_input":"2024-08-11T17:05:14.452417Z","iopub.status.idle":"2024-08-11T17:05:14.461125Z","shell.execute_reply.started":"2024-08-11T17:05:14.452386Z","shell.execute_reply":"2024-08-11T17:05:14.460313Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Calculate class weights on basis of training split dataframe and print weight tensor\nclass_weights = get_class_weights(df_train)\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.462328Z","iopub.execute_input":"2024-08-11T17:05:14.462656Z","iopub.status.idle":"2024-08-11T17:05:14.479689Z","shell.execute_reply.started":"2024-08-11T17:05:14.462627Z","shell.execute_reply":"2024-08-11T17:05:14.478836Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"tensor([0.6070, 0.9407, 0.8100, 0.9791, 0.6998, 0.9633])\n","output_type":"stream"}]},{"cell_type":"code","source":"EPOCHS = 20\n\n# AdamW optimizer with a linear learning rate scheduler\noptimizer = AdamW(fn_detector.parameters(), lr=3e-5, correct_bias=False)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=len(train_data_loader) * EPOCHS\n)\n\n# Weighted CrossEntropyLoss assigned to device\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.481007Z","iopub.execute_input":"2024-08-11T17:05:14.481478Z","iopub.status.idle":"2024-08-11T17:05:14.494135Z","shell.execute_reply.started":"2024-08-11T17:05:14.481452Z","shell.execute_reply":"2024-08-11T17:05:14.493220Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model training procedure","metadata":{}},{"cell_type":"code","source":"def train_model(model, data_loader, loss_function, optimizer,scheduler, num_examples):\n    print(\"Training model in progress...\")\n    print(\"-\" * 10)\n    \n    # Putting model in training condition including regularization layers\n    model = model.train()\n    \n    # Saving training lossses per epoch and initalizing correct prediction count\n    train_losses = []\n    correct_preds = 0\n    \n    # Iteration over data (batches) contained in data split set DataLoader\n    for data in tqdm(data_loader):\n\n        # Initializing post title input_ids, attention_mask, \n        \n        input_ids = data[\"input_ids\"]\n        attention_mask = data[\"attention_mask\"]\n        images = data[\"image\"]\n        labels = data[\"label\"]\n\n        # Feeding input data to Title-Image DistilFND\n        outputs = model(\n                title_input_ids = input_ids,\n                title_attention_mask = attention_mask,\n                image = images\n        )\n\n        # Final Softmax layer returns class predictions per sample in batch\n        # Highest probability value resembles class prediction and is assigned to preds variable\n        _, preds = torch.max(outputs, dim=1)\n\n        # Training loss is calculated by applying weighted Cross Entropy Loss\n        # on comparison between predicted label and ground truth label\n        train_loss = loss_function(outputs, labels)\n\n        # Counting correct model predictions and incrementing correct prediction count\n        correct_preds += torch.sum(preds == labels)\n        # Append training loss of current epoch to list of training losses\n        train_losses.append(train_loss.item())\n        # Initialize backpropagation to adjust model weights / parameters\n        train_loss.backward()\n        # Normalize gradient values to regularize parameter update\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        # Perform parameter update based on current gradient value\n        optimizer.step()\n        # Moving scheduler to next step / iteration\n        scheduler.step()\n        # Zero out current gradients to initialize fresh optimizer state for next epoch\n        optimizer.zero_grad()\n            \n    # Return train_acc and train_loss values\n    return correct_preds.double() / num_examples, np.mean(train_losses)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.498546Z","iopub.execute_input":"2024-08-11T17:05:14.498872Z","iopub.status.idle":"2024-08-11T17:05:14.508503Z","shell.execute_reply.started":"2024-08-11T17:05:14.498847Z","shell.execute_reply":"2024-08-11T17:05:14.507560Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, data_loader, loss_function, num_examples):\n    print(\"Validating model in progress...\")\n    print(\"-\" * 10)\n    \n    # Switching off regularization and normalization layers for evaluation mode\n    # Hence, no model parameters are adjusted. Model is evaluated in current state\n    model = model.eval()\n    \n    # Saving validation loss per epoch\n    val_losses = []\n    correct_preds = 0\n    \n    # Skipping gradient calulation over weights --> Not needed, because\n    # no parameters are updated and significantly speeds up iteration over samples batches\n    with torch.no_grad():\n        # Iteration over data (batches) contained in data split set DataLoader\n        for data in tqdm(data_loader):\n            \n            # Initializing post title input_ids, attention_mask, \n            # image data and label per Reddit-Post and assigning to device\n            input_ids = data[\"input_ids\"]\n            attention_mask = data[\"attention_mask\"]\n            images = data[\"image\"]\n            labels = data[\"label\"]\n            \n            # Feeding input data to Title-Image DistilFND state in current epoch\n            outputs = model(\n                    title_input_ids = input_ids,\n                    title_attention_mask = attention_mask,\n                    image = images\n            )\n            \n            # Final Softmax layer returns class predictions per sample in batch\n            # Highest probability value resembles class prediction and is assigned to preds variable\n            _, preds = torch.max(outputs, dim=1)\n            \n            # Validation loss is calculated by applying weighted Cross Entropy Loss\n            # on comparison between predicted label and ground truth label\n            val_loss = loss_function(outputs, labels)\n            \n            # Counting correct model predictions and incrementing correct prediction count\n            correct_preds += torch.sum(preds == labels)\n            \n            # Appending current validation loss per batch\n            # to list of validation losses per epoch\n            val_losses.append(val_loss.item())\n    \n    # Returns val_acc and val_loss values\n    return correct_preds.double() / num_examples, np.mean(val_losses)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.509857Z","iopub.execute_input":"2024-08-11T17:05:14.510172Z","iopub.status.idle":"2024-08-11T17:05:14.524538Z","shell.execute_reply.started":"2024-08-11T17:05:14.510148Z","shell.execute_reply":"2024-08-11T17:05:14.523702Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Initializing training history dictionary and best_accuracy variable\nhistory = defaultdict(list)\nbest_accuracy = 0\n\n# Iteration times the total number of epochs\nfor epoch in range(EPOCHS):\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(\"-\" * 10)\n\n    # Calling train_model() function, returns train_acc and train_loss\n    train_acc, train_loss = train_model(\n        fn_detector,\n        train_data_loader,\n        loss_function,\n        optimizer,\n        scheduler,\n        len(df_train)\n    )\n\n    # Print train_loss and train_acc values for current epoch\n    print(f\"Train loss {train_loss} | Accuracy {train_acc}\")\n    print()\n\n    # Calling evaluate_model() function, returns val_acc and val_loss\n    val_acc, val_loss = evaluate_model(\n            fn_detector,\n            validate_data_loader,\n            loss_function,\n            device,\n            len(df_validate)\n    )\n\n    # Print val_loss and val_acc values for current epoch\n    print(f\"Val   loss {val_loss} | Accuracy {val_acc}\")\n    print()\n\n    # Save current values of train_acc, val_acc, train_loss and val_loss\n    # in respective keys of history dictionary for later analysis\n    history[\"train_acc\"].append(train_acc)\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_acc\"].append(val_acc)\n    history[\"val_loss\"].append(val_loss)\n\n   \n# Print output when training procedure completed\nprint()\nprint(\"Completed Training!\")\nprint(\"-\" * 20)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:05:14.525528Z","iopub.execute_input":"2024-08-11T17:05:14.525843Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n----------\nTraining model in progress...\n----------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/594 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6e0dd331f243b991d5df2eba1bc74d"}},"metadata":{}}]}]}